---
title: "HW7"
author: "Tyler Christians"
date: "2022-10-24"
output: pdf_document
---


Problem 1.

The best way to decribe it would be the constraints, or numerical rules behind lasso regression makes the shape of a diamond. When Lasso creates a model, if the model hits a cortner of that diamond it can lead to the shrinking of the betas which can eventualyl hit 0. In ridge regression the restraints create a circle which makes it nearly impossible for shrinkage to lead to betas being brought down to 0


Problem 2.


a.

Training MSE will steadily increase because of the decrease in flexibility of the model this leads to an increase of training MSE


b.

Test MSE will increase intitially and then begin to decrease. This is because the model will be extremely flexible at the beginning leading to overfitting on the training data. as lambda increases the model becomes less flexible.


c.

Variance will decrease because flexibility is decreasing as well


d.

Bias squared will increase because model flexibility is decreasing


e.

Irreducible error will remain constant because it doesn't change with complexity


Problem 3.


a.

```{r echo=TRUE, warning=FALSE}
library(ISLR2)
Hitters = na.omit(Hitters)
n = nrow(Hitters) #there are 263 observations
x = model.matrix(Salary ~.,data=Hitters)[,-1] #19 predictors
Y = Hitters$Salary
set.seed(1)
train = sample(1:nrow(x), nrow(x)/2)
test=(-train)
Y.test = Y[test]

```


b.

```{r echo=FALSE, warning=FALSE}
library(glmnet)

grid = 10^seq(10,-2,length=100)


cv.out.ridge = cv.glmnet(x[train, ], Y[train], alpha = 0, lambda = grid) 

plot(cv.out.ridge)
lambdaridgemin = cv.out.ridge$lambda.min
print(paste("LambdaRidgeMin =", round(lambdaridgemin,4)))
```


c.

```{r echo=FALSE, warning=FALSE}

LambdaRidge1SE <- cv.out.ridge$lambda.1se


print(paste("LambdaRidge1SE =", round(cv.out.ridge$lambda.1se, 4)))
```

d.

```{r echo=FALSE}
cv.out.lasso = cv.glmnet(x[train, ], Y[train], alpha = 1, lambda = grid) 

lambdaLassomin = cv.out.lasso$lambda.min

print(paste("LambdaLassoMin =", round(lambdaLassomin,4)))

LambdaLasso1SE <- cv.out.lasso$lambda.1se

print(paste("LambdaLasso1SE =", round(cv.out.lasso$lambda.1se, 4)))
```


e.

```{r echo=FALSE}
ridge.pred.min = predict(cv.out.ridge, s = lambdaridgemin, newx = x[test, ])
print(paste("The MSE for LambdaRidgeMin =", round(mean((ridge.pred.min - Y.test)^2), 1)))

ridge.pred.1SE = predict(cv.out.ridge, s = LambdaRidge1SE, newx = x[test, ])
print(paste("The MSE for LambdaRidge1SE =", round(mean((ridge.pred.1SE - Y.test)^2), 1)))

lasso.pred.min = predict(cv.out.lasso, s = lambdaLassomin, newx = x[test, ])
print(paste("The MSE for LambdaLassoMin =", round(mean((lasso.pred.min - Y.test)^2), 1)))

lasso.pred.1SE = predict(cv.out.lasso, s = LambdaLasso1SE, newx = x[test, ])
print(paste("The MSE for LambdaLasso1SE =", round(mean((lasso.pred.1SE - Y.test)^2), 1)))
```


The model that creates the best test MSE is Ridge regression using the minimum lambda. This could be due to there being a subset of true regression coefficients that are small.


f.

```{r echo=FALSE}
final.ridge.min <- glmnet(x,Y,alpha=0,lambda=lambdaridgemin)
coef(final.ridge.min)

final.ridge.1se <- glmnet(x,Y,alpha=0,lambda=LambdaRidge1SE)
coef(final.ridge.1se)
```

```{r echo=FALSE}
final.lasso.min <- glmnet(x,Y,alpha=1,lambda=lambdaLassomin)
coef(final.lasso.min)

final.lasso.1se <- glmnet(x,Y,alpha=1,lambda=LambdaLasso1SE)
coef(final.lasso.1se)
```

The coeffiecients for both the ridge and lasso models are much closer or are 0 in the one standard error model than the the minimum lambda models. The same can be said that the lasso models have coefficients much closer to 0 than the ridge regression model.


g.

I would reccomend focusing on.

Hits
Home Runs
Runs
RBI
Years



Problem 3.


a.
```{r warning=FALSE, include=FALSE}
library(ISLR2)

df <- ISLR2::Boston

smp_size <- floor(0.9 * nrow(df))

index <- sample(seq_len(nrow(df)), size = smp_size)

train <- df[index, ]

test <- df[-index,]

model0 <- lm(crim~1, data = train)
modelfull <- lm(crim~., data = train)

library(MASS)
steps <- stepAIC(modelfull,scope=list(lower=model0, upper=modelfull), direction="backward")
```

```{r echo=FALSE}
model1 <- steps

model1 <- lm(log(crim) ~ zn + nox + dis + ptratio + lstat + medv, data = train)

summary(model1)

print(paste("The test MSE for my lm is", round(mean((test$crim - predict.lm(model1, test))^2),2)))

plot(model1, which = c(1,3))

model1 <- lm(log(crim)~ zn + nox + dis + ptratio + lstat + medv, data = df)
```

I chose to use step AIC as the basis for figuring out which model to use, I like to use it because it does a great analysis on finding the coeeeficients, model size, and what predictors to use. In the original model there were problems with linearity so I made it a log transformation. There was also a problem with the constant variance which still isn't perfect but was made better by the log transformation.



b.

```{r echo=FALSE}
n = nrow(df) #there are 263 observations
x = model.matrix(crim ~.,data=df)[,-1] #19 predictors
Y = df$crim
set.seed(1)
train = sample(1:nrow(x), nrow(x)*.9)
test=(-train)
Y.test = Y[test]

grid = 10^seq(10,-2,length=100)

cv.out.ridge = cv.glmnet(x[train, ], Y[train], alpha = 0, lambda = grid) 

lambdaRidgemin = cv.out.ridge$lambda.min

LambdaRidge1SE <- cv.out.ridge$lambda.1se

model2a <- glmnet(x,Y, alpha = 0,lambda = lambdaRidgemin)

model2b <- glmnet(x,Y, alpha = 0, lambda = LambdaRidge1SE)


ridge.pred.min = predict(cv.out.ridge, s = lambdaridgemin, newx = x[test, ])
print(paste("The test MSE for LambdaRidgeMin =", round(mean((ridge.pred.min - Y.test)^2), 1)))

ridge.pred.1SE = predict(cv.out.ridge, s = LambdaRidge1SE, newx = x[test, ])
print(paste("The test MSE for LambdaRidge1SE =", round(mean((ridge.pred.1SE - Y.test)^2), 1)))

model3a <- glmnet(x,Y,alpha=0, lambda=lambdaRidgemin)
coef(model3a)

model3b <- glmnet(x,Y,alpha=0, lambda=LambdaRidge1SE)
coef(model3b)
```


c.

```{r echo=FALSE}
n = nrow(df) #there are 263 observations
x = model.matrix(crim ~.,data=df)[,-1] #19 predictors
Y = df$crim
set.seed(1)
train = sample(1:nrow(x), nrow(x)*.9)
test=(-train)
Y.test = Y[test]

grid = 10^seq(10,-2,length=100)

cv.out.lasso = cv.glmnet(x[train, ], Y[train], alpha = 1, lambda = grid) 

lambdaLassomin = cv.out.lasso$lambda.min

LambdaLasso1SE <- cv.out.lasso$lambda.1se

model3a <- glmnet(x,Y, alpha = 1,lambda = lambdaLassomin)

model3b <- glmnet(x,Y, alpha = 1, lambda = LambdaLasso1SE)


lasso.pred.min = predict(cv.out.lasso, s = lambdaLassomin, newx = x[test, ])
print(paste("The test MSE for LambdaLassoMin =", round(mean((lasso.pred.min - Y.test)^2), 1)))

lasso.pred.1SE = predict(cv.out.lasso, s = LambdaLasso1SE, newx = x[test, ])
print(paste("The test MSE for LambdaLasso1SE =", round(mean((lasso.pred.1SE - Y.test)^2), 1)))

model3a <- glmnet(x,Y,alpha=1,lambda=lambdaLassomin)
coef(model3a)

model3b <- glmnet(x,Y,alpha=1,lambda=LambdaLasso1SE)
coef(model3b)
```


d.

I propose using the Lasso model with the lowest lambda because of its low test MSE and wide range of predictors. It doesn't use all of them however it uses most.



Problem 4


a.

```{r echo=FALSE}
print(paste("Estimate of the mean of medv:", round(mean(df$medv),2)))
```


b.

```{r echo=FALSE}
print(paste("Estimate of the standard error for medv:", round((sd(df$medv)/sqrt(508)),3)))
```


c.

```{r echo=FALSE}
set.seed(1)
library(boot)

meanFunc <- function(x,i){mean(x[i])}

boot(df$medv, meanFunc, 5000)
```

The results are very similar only being different by about 0.004


d.

```{r echo=FALSE}
set.seed(1)
b1<-boot(df$medv,function(u,i) mean(u[i]),R=1000)
boot.ci(b1,type="norm")
```
The confidence interval is similar to the analytical output and contains the value of the analytical output within the 95% confidence interval


e.

```{r echo=FALSE}
print(paste("estimate for median:", median(df$medv)))
```


g.


```{r echo=FALSE}
print(paste("estimate for the 10th percentile", quantile(df$medv, 0.1)))
```



Problem 5.


a.

The probability is 1/n because there are a total of n possibilities and the chance that it isn't the first one for example is 1/n


b.

The probability is 1 - 1/n because it could be any other observation than the one randomly chosen, the first one for example has chance 1/n to be chosen so the chance that it isn't is the chance of any other thing happening


c.

The probability is (1 - 1/n)^n this is because the chance that the jth observation is not in the sample is the probability that all the observations are not the jth observation.

d.

1 - (1 - 1/5)^5


e.

1 - (1 - 1/100)^100


f.

1 - (1-1/10000)^10000


g.

```{r}
x <- 1:100000
y <- rep(NA,100000)

for (i in 1:100000) {
  y[i] <- 1 - (1 - 1/x[i])^x[i]
}

plot(x, y)
```



h.

```{r}
results <- rep(NA, 10000)
for(i in 1:10000){
results[i] <- sum(sample(1:100, rep=TRUE) == 5) > 0
}
mean(results)

```

Each time this is run it results in around .63-.64. This means that the chances of one of the jth sample appearing in a sample of 100 is around .63.